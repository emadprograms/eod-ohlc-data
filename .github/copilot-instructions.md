# GitHub Copilot Instructions

This document provides a comprehensive guide for AI coding agents to understand and contribute to the `eod-ohlc-data` project. Adhering to these instructions is critical for maintaining the project's architecture and quality.

## 1. Project Overview

This is a Streamlit-based web application designed as a sophisticated AI analysis pipeline for financial markets. Its primary purpose is to automate the daily workflow of a trader by:

1.  **Processing** raw intraday (5-minute) market data into a structured, quantitative summary.
2.  **Analyzing** this summary in the context of historical data and a global market narrative using the Google Gemini API.
3.  **Generating** daily "Battle Cards" for individual stocks and a global "Economy Card" for market context.
4.  **Screening** for tactical trading opportunities based on pre-defined criteria (e.g., proximity to major support/resistance).

The application is divided into distinct pages (Streamlit's multi-page app format) that represent different stages of the analysis pipeline.

## 2. Core Components & Architecture

The project follows a multi-stage pipeline architecture. Understanding the role of each file is crucial.

### `database_setup.py`
- **Purpose**: Defines the SQLite database schema (`analysis_database.db`).
- **Tables**:
    - `stocks`: The primary table. It stores the AI-generated "Company Overview Card" (a JSON object) and static, user-provided `historical_level_notes` for each ticker.
    - `data_archive`: A historical log of the raw text summaries generated by the processor.
    - `market_context`: A special table that holds the single, global "Economy Card" JSON.
- **Agent's Role**: Do not modify this file unless explicitly asked to alter the database schema. All database interactions in the app should be done via `sqlite3` connections in the respective workflow functions.

### `pages/processor.py`
- **Purpose**: This is the **first stage** of the pipeline. It is a self-contained Streamlit page responsible for data collection and initial processing.
- **Core Logic**:
    1.  Fetches 5-minute intraday data for a list of tickers using `yfinance`.
    2.  Performs quantitative analysis (VWAP, Volume Profile, Opening Range).
    3.  Generates a structured, multi-line string summary for each ticker.
- **Key Function**: `generate_analysis_text(tickers_to_process, analysis_date)` is the heart of this file. It performs the analysis and returns the formatted string.
- **Agent's Role**:
    - This file's logic should remain focused on **objective, quantitative data processing**.
    - **DO NOT** add any AI (Gemini) calls to this file.
    - This file is now used as both a standalone Streamlit page and a **module** imported by `2_pipeline_engine.py`.

### `pages/2_pipeline_engine.py`
- **Purpose**: This is the **main application and orchestration layer**. It contains the user-facing workflows that consume the data from `processor.py` and orchestrate the AI analysis.
- **Architecture**: This file is organized into tabs, each representing a key workflow.
- **Data Flow**:
    1.  The user clicks a "Run Processor" button in the "EOD Runner" tab.
    2.  This triggers a call to the imported `generate_analysis_text` function from `pages/processor.py`.
    3.  The returned string is placed into a `st.text_area`.
    4.  The user then clicks "Update EOD Note", which takes this text, combines it with historical context from the database, and sends it to the Gemini API to generate an updated "Company Overview Card" JSON.
    5.  The new JSON is saved back to the `stocks` table in the database.
- **Agent's Role**:
    - This is where all **AI (Gemini) calls** and **database interactions** happen.
    - When adding new features, they will most likely be implemented as new functions or UI elements within this file.
    - **CRITICAL**: The integration between `processor.py` and `2_pipeline_engine.py` is via a **direct Python import**. Do NOT re-introduce `subprocess` or any other command-line execution methods.

### `app.py`
- **Purpose**: The main entry point for the Streamlit application. It contains minimal code and primarily serves to launch the app.
- **Agent's Role**: You will rarely, if ever, need to modify this file.

## 3. Key Workflows & Development Patterns

### Workflow 1: The EOD Runner
- **Goal**: Update the "Company Overview Card" for a single stock.
- **Steps**:
    1.  Run the processor for stocks (`generate_analysis_text`).
    2.  Copy the output into the "Raw Text" area.
    3.  Optionally add a "Macro/Company News Summary".
    4.  Click "Update EOD Note".
    5.  The `update_stock_note` function orchestrates the AI call and database update.

### Workflow 2: The Pre-Flight Check
- **Goal**: Identify and prepare tactical plans for a curated list of stocks before the market opens.
- **Steps**:
    1.  **Scan**: The `run_proximity_filter` function scans all tickers in the database to find stocks trading near their major support/resistance levels.
    2.  **Curate**: The user selects a subset of these tickers from a `st.multiselect` box.
    3.  **Generate**: The `generate_premarket_tactical_cards` function is called **only for the selected tickers**. This is an expensive AI operation and must not be run for all tickers.

### Pattern: Gemini API Usage
- The function `call_gemini_api` is a robust wrapper that handles API key rotation (from `st.secrets`) and exponential backoff for retries.
- **Always** use this wrapper for any new AI calls.
- System prompts should be concise and directive, clearly explaining the AI's persona and task.

### Pattern: JSON Schemas
- The `DEFAULT_COMPANY_OVERVIEW_JSON` and `DEFAULT_ECONOMY_CARD_JSON` serve as the canonical schemas for the AI's output.
- When creating or updating prompts, always reinforce that the AI must adhere to this structure. The prompts should explicitly state which fields are "AI Updates" and which are "READ-ONLY" or preserved.

## 4. What to Avoid (Don'ts)

- **DO NOT** use `subprocess.run` to call `processor.py`. The direct import method (`from pages.processor import ...`) is the established and correct pattern.
- **DO NOT** mix concerns. `processor.py` is for quantitative data; `2_pipeline_engine.py` is for AI analysis and database state management.
- **DO NOT** perform expensive operations (like AI calls) on a full list of tickers without an explicit user curation step. The "Pre-Flight Check" workflow is the template for this pattern.
- **DO NOT** hardcode API keys. They must be loaded from `st.secrets.toml`.
- **DO NOT** modify the database schema (`database_setup.py`) without a specific request.
